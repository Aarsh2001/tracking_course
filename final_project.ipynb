{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "final_project.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-Zy5R377k8C",
        "colab_type": "text"
      },
      "source": [
        "# Welcome to the Final Project!\n",
        "\n",
        "Course: https://jeremycohen.podia.com/tracking-obstacles-using-computer-vision\n",
        "\n",
        "In this project, you will learn to associate bounding boxes on multiple frames using the Hungarian Algorithm!\n",
        "\n",
        "\n",
        "You will work on 3 aspects of the **multi-object tracker**:\n",
        "\n",
        "*   Use YOLO and launch an object detection algorithm\n",
        "*   Use The Hungarian Algorithm and associate the boxes\n",
        "*   Improve the algorithm to avoid false positives and false negatives\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VbG8vs9u8OOg",
        "colab_type": "text"
      },
      "source": [
        "This is a part included to link your Google Colab file (.ipynb) to your Google Drive folder.\n",
        "\n",
        "If you don't work on Colab, you won't need these."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZPW9hbcuYxqO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 184
        },
        "outputId": "58db0360-7140-4381-fe4e-120e17835114"
      },
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=False)\n",
        "\n",
        "os.chdir(\"/content/drive/My Drive/Think Autonomous/SDC Course/Tracking\")\n",
        "!ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n",
            "association_hungarian.ipynb\t     Images\t       yolo_for_tracking.py\n",
            "association_hungarian_Starter.ipynb  Output\t       yolo.ipynb\n",
            "final_project.ipynb\t\t     __pycache__       yolo_Starter.ipynb\n",
            "final_project_Starter.ipynb\t     Tracking.gslides  Yolov3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZcNCkytu3ykt",
        "colab_type": "text"
      },
      "source": [
        "# 1 - Detection\n",
        "\n",
        "In order to make multi-object tracking work, we will need to do a detection step. The tracking will heavily rely on the detector, it better be good.\n",
        "\n",
        "We will choose the [YOLO algorithm](https://pjreddie.com/darknet/yolo/) that is both accurate and fast.\n",
        "\n",
        "<img src=\"https://miro.medium.com/max/1446/1*YpNE9OQeshABhBgjyEXlLA.png\" width=\"500\">\n",
        "\n",
        "\n",
        "Eventually, we want bounding box detection\n",
        "\n",
        "<img src=\"https://pjreddie.com/media/image/Screen_Shot_2018-03-24_at_10.48.42_PM.png\" width=\"500\">\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UStSXSC3RKue",
        "colab_type": "text"
      },
      "source": [
        "## Import Libraries and Test Images\n",
        "\n",
        "Let's import the libraries and test images.<p>\n",
        "I took a video and wrote a short script to take a picture every 7 frame of the image. Instead of working at **60 FPS** (recording frame rate), consider you have an algorithm working at 60/7 or about **9 frame per second**.<p>_\n",
        "\n",
        "**Why the cut ?**<p>\n",
        "YOLO is very fast, it can work at 60 FPS.\n",
        "For tracking to be a bit challenging, let's not have 99% IOU every time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9n24pesxqdTP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Imports\n",
        "import matplotlib.image as mpimg\n",
        "import matplotlib.pyplot as plt\n",
        "import copy\n",
        "import pickle\n",
        "import cv2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cJ6bw_4Rqh3T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Load the Images\n",
        "dataset_images = pickle.load(open('Images/images_tracking.p', \"rb\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "duVgFEYHfRsM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def visualize_images(input_images):\n",
        "    fig=plt.figure(figsize=(100,100))\n",
        "\n",
        "    for i in range(len(input_images)):\n",
        "        fig.add_subplot(1, len(input_images), i+1)\n",
        "        plt.imshow(input_images[i])\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ref3X-llqt4o",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 368
        },
        "outputId": "282a114f-8afa-4103-e772-a749b98d09f6"
      },
      "source": [
        "visualize_images(dataset_images)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9P5YlyD3UZmq",
        "colab_type": "text"
      },
      "source": [
        "## Run Initial Obstacle Detection - Modify the YOLO file\n",
        "\n",
        "We will need to modify the original yolo.py file. Go to this file, **duplicate it**, and **modify the duplicate online** using Google Drive's text editor. <p>_\n",
        "\n",
        "**What modifications should you do?**<p>\n",
        "The current *inference()* function outputs an image.\n",
        "To work with the hungarian algorithm, we will need the bounding box.<p>\n",
        "* **Modify the postprocess()** function as well as the **inference() function** to **return the bounding box**.\n",
        "* Then, we will only work with the original image and the bounding boxes\n",
        "* Call the new file **yolo_modified.py**\n",
        " and import it"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S2QETWZhM3j8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "aa2297e9-de2e-4038-b29a-7b2ce550f379"
      },
      "source": [
        "### Run obstacle detection for the images\n",
        "from yolo_for_tracking import *\n",
        "\n",
        "result_images = [] # Empty list for output images\n",
        "result_boxes = [] # Empty list for output boxes\n",
        "\n",
        "# Initiliaze an object detector\n",
        "detector = YOLO()\n",
        "\n",
        "images = copy.deepcopy(dataset_images)\n",
        "\n",
        "# For every image, run a detector using the inference() function\n",
        "for img in images:\n",
        "    result, boxes = detector.inference(img)\n",
        "    result_images.append(result)\n",
        "    result_boxes.append(boxes)\n",
        "\n",
        "print(result_boxes)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "[[[474, 207, 295, 158], [147, 213, 74, 51], [265, 214, 65, 53], [112, 212, 12, 22], [346, 203, 42, 43], [216, 214, 38, 31], [149, 215, 73, 50], [265, 221, 65, 43]], [[469, 208, 302, 155], [143, 213, 73, 51], [259, 214, 68, 53], [103, 213, 13, 22], [253, 216, 16, 12], [339, 202, 43, 45], [145, 210, 73, 49], [212, 216, 40, 30], [262, 222, 63, 43]], [[471, 206, 296, 155], [148, 212, 72, 51], [264, 211, 69, 56], [344, 202, 44, 45], [654, 205, 18, 24], [153, 209, 68, 52], [217, 213, 39, 30], [265, 220, 65, 43]], [[471, 214, 293, 146], [153, 214, 68, 51], [269, 213, 64, 57], [102, 212, 13, 25], [681, 204, 21, 39], [218, 217, 42, 29], [342, 209, 51, 42], [425, 216, 11, 25], [711, 213, 19, 43], [157, 218, 64, 46], [269, 222, 63, 44]], [[477, 211, 283, 143], [158, 213, 72, 52], [273, 215, 70, 50], [352, 204, 43, 41], [395, 213, 9, 18], [425, 211, 9, 19], [446, 211, 11, 20], [225, 213, 41, 31], [721, 202, 24, 57], [162, 216, 62, 48], [275, 220, 65, 46], [752, 204, 15, 73]], [[466, 208, 265, 144], [148, 214, 74, 49], [91, 207, 15, 28], [339, 202, 45, 44], [388, 210, 9, 23], [424, 209, 9, 25], [447, 210, 8, 19], [216, 212, 44, 31], [266, 211, 63, 41], [742, 189, 24, 89], [151, 217, 71, 44]], [[461, 204, 255, 147], [146, 206, 72, 54], [266, 205, 65, 55], [80, 206, 15, 24], [215, 209, 44, 31], [323, 211, 16, 18], [339, 202, 46, 44], [390, 210, 8, 23], [425, 209, 8, 20], [147, 208, 74, 51], [263, 210, 66, 43]], [[456, 222, 257, 134], [150, 215, 74, 50], [268, 213, 66, 55], [82, 208, 14, 26], [326, 215, 13, 16], [220, 215, 41, 30], [345, 208, 41, 44], [395, 219, 8, 21], [444, 220, 8, 22], [464, 221, 8, 18], [642, 218, 16, 24], [154, 216, 69, 48], [268, 222, 65, 44]], [[462, 219, 258, 134], [167, 219, 71, 54], [281, 219, 64, 56], [92, 219, 18, 26], [340, 222, 17, 17], [355, 213, 42, 42], [410, 222, 9, 24], [449, 222, 8, 18], [462, 221, 9, 22], [484, 222, 10, 19], [670, 218, 22, 36], [167, 218, 73, 55], [236, 223, 38, 30], [283, 222, 64, 43]]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bHvtTDrEVr_7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Print the results and the detected boxes\n",
        "visualize_images(result_images)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MUIbq5XkXGaY",
        "colab_type": "text"
      },
      "source": [
        "## One Obstacle - One Color\n",
        "**Last Step!** <p>\n",
        "We now one one color per bounding box! All cars in blue is useless! <p>\n",
        "We will create an Obstacle class that we will modify.\n",
        "Each detected obstacle should have:\n",
        "* an id\n",
        "* a current bounding box\n",
        "* a previous bounding box<p>_\n",
        "\n",
        "**In the end, we will draw a bounding box based on the id.** <p>\n",
        "If the id changes, the color will change also."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a99w3HIF1MLL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Obstacle():\n",
        "    def __init__(self, idx, box):\n",
        "        \"\"\"\n",
        "        Init function. The obstacle must have an id and a box.\n",
        "        \"\"\"\n",
        "        self.idx = idx\n",
        "        self.box = box"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ly04QA_gOMJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def id_to_color(idx):\n",
        "    \"\"\"\n",
        "    Random function to convert an id to a color\n",
        "    Do what you want here but keep numbers below 255\n",
        "    \"\"\"\n",
        "    blue = idx*5 % 256\n",
        "    green = idx*36 %256\n",
        "    red = idx*23 %256\n",
        "    return (red, green, blue)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yWacVf_5gShC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def main():\n",
        "    \"\"\"\n",
        "    Main function.\n",
        "    You already ran the detector on all 9 images. The variable is result_boxes.\n",
        "    Use this to assign an id and draw a rectangle based on the id.\n",
        "    \"\"\"\n",
        "    idx = 0\n",
        "    obstacles = []\n",
        "    result_images_2 = copy.deepcopy(dataset_images) # Copy the image without modifying the dataset\n",
        "    for j, boxes in enumerate(result_boxes): # loop through all images\n",
        "        for i, box in enumerate(boxes): # loop through all boxes\n",
        "            obs = Obstacle(idx, box)\n",
        "            left, top, width, height = box\n",
        "            right = left+width\n",
        "            bottom = top+height\n",
        "            cv2.rectangle(result_images_2[j], (left, top), (right, bottom), id_to_color(idx+i), thickness=10)\n",
        "            idx +=1\n",
        "            obstacles.append(obs)\n",
        "    return result_images_2\n",
        "\n",
        "result_images_2 = main()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Eq18R7WYXkD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 368
        },
        "outputId": "e4ae6fd4-d7d7-498d-e197-60159f93dc37"
      },
      "source": [
        "## Print the results\n",
        "visualize_images(result_images_2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NmsuPGWAbHvd",
        "colab_type": "text"
      },
      "source": [
        "We did it!<p>...<p>\n",
        "\n",
        "But as you can see, the colors are not kept along the images. **We don't have active tracking yet**!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BCEQikXa30Cx",
        "colab_type": "text"
      },
      "source": [
        "# 2 - Association\n",
        "\n",
        "We now have a detection algorithm working! Congratulations!\n",
        "The next step is to **match the detections** from one frame to another and **keep the color along the 9 images**.<p>\n",
        "It should be dynamic and **work no matter the number of images**. In the end, we'll apply **this algorithm on a video**.\n",
        "\n",
        "Eventually, we'll want a good association system\n",
        "\n",
        "![Texte alternatif…](https://miro.medium.com/proxy/0*yN9MllhmuglJORss.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jD-ZE43rcVvW",
        "colab_type": "text"
      },
      "source": [
        "## Metrics\n",
        "\n",
        "The first thing we'll need is to define a metric!\n",
        "\n",
        "* If you **don't want a big challenge**, the **IOU cost** will do just fine!\n",
        "* If you want a **medium challenge**, you can try to **implement [this paper](https://arxiv.org/pdf/1709.03572.pdf)**. Read carefully page 19-20 and try to implement these costs with IOU. It will filter out incoherent boxes.\n",
        "* If you want the **biggest challenge**, try to **code [Deep SORT](https://arxiv.org/pdf/1703.07402.pdf)** and associate Deep Convolutional features to it. <p>\n",
        "\n",
        "In the end, you should have a **single number in the cost matrix**. And it should be representative of the cost, like IOU is."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n-grOk3_hHSc",
        "colab_type": "text"
      },
      "source": [
        "### IOU COST"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0NpEcw8L5HnS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def convert_data(box):\n",
        "    \"\"\"\n",
        "    Convert data from (x1,y1, w, h) to (x1,y1,x2,y2)\n",
        "    \"\"\"\n",
        "    x1 = box[0]\n",
        "    x2 = box[0] + box[2]\n",
        "    y1 = box[1]\n",
        "    y2 = box[1]+box[3]\n",
        "    return x1,y1,x2,y2\n",
        "\n",
        "def box_iou(box1, box2):\n",
        "    \"\"\"\n",
        "    Computer Intersection Over Union cost\n",
        "    \"\"\"\n",
        "    box1 = convert_data(box1)\n",
        "    box2 = convert_data(box2)\n",
        "    xA = max(box1[0], box2[0])\n",
        "    yA = max(box1[1], box2[1])\n",
        "    xB = min(box1[2], box2[2])\n",
        "    yB = min(box1[3], box2[3])\n",
        "\n",
        "    inter_area = max(0, xB - xA + 1) * max(0, yB - yA + 1) #abs((xi2 - xi1)*(yi2 - yi1))\n",
        "    # Calculate the Union area by using Formula: Union(A,B) = A + B - Inter(A,B)\n",
        "    box1_area = (box1[2] - box1[0] + 1) * (box1[3] - box1[1] + 1) #abs((box1[3] - box1[1])*(box1[2]- box1[0]))\n",
        "    box2_area = (box2[2] - box2[0] + 1) * (box2[3] - box2[1] + 1) #abs((box2[3] - box2[1])*(box2[2]- box2[0]))\n",
        "    union_area = (box1_area + box2_area) - inter_area\n",
        "    # compute the IoU\n",
        "    iou = inter_area/float(union_area)\n",
        "    return iou"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XRVXGejWhL_S",
        "colab_type": "text"
      },
      "source": [
        "### Exponential, Linear, And IOU Costs\n",
        "\n",
        "Use this paper to code the solution: https://arxiv.org/pdf/1709.03572.pdf"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OK7LfLBthi8r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from math import sqrt, exp\n",
        "\n",
        "def check_division_by_0(value, epsilon=0.01):\n",
        "    if value < epsilon:\n",
        "        value = epsilon\n",
        "    return value\n",
        "\n",
        "def hungarian_cost(old_box, new_box, iou_thresh = 0.3, linear_thresh = 10000, exp_thresh = 0.5):\n",
        "        w1 = 0.5\n",
        "        w2 = 1.5\n",
        "        (_, h, w, _) = np.array(dataset_images).shape\n",
        "        # IOU COST\n",
        "        iou_cost = box_iou(old_box, new_box)\n",
        "        \n",
        "        ### Sanchez-Matilla et al COST\n",
        "        Q_dist = sqrt(pow(w,2)+pow(h,2)) # First real-life Pythagore use in your life\n",
        "        Q_shape = w*h\n",
        "        distance_term = Q_dist/check_division_by_0(sqrt(pow(old_box[0] - new_box[0], 2)+pow(old_box[1] -new_box[1],2)))\n",
        "        shape_term = Q_shape/check_division_by_0(sqrt(pow(old_box[2] - new_box[2], 2)+pow(old_box[3] - new_box[3],2)))\n",
        "        linear_cost = distance_term*shape_term\n",
        "\n",
        "        ## YUL et al COST\n",
        "        a= (old_box[0] - new_box[0])/check_division_by_0(old_box[2])\n",
        "        a_2 = pow(a,2)\n",
        "        b = (old_box[1] - new_box[1])/check_division_by_0(old_box[3])\n",
        "        b_2 = pow(b,2)\n",
        "        ab = (a_2+b_2)*w1*(-1)\n",
        "        c = abs(old_box[3] - new_box[3])/(old_box[3]+new_box[3])\n",
        "        d = abs(old_box[2]-new_box[2])/(old_box[2]+new_box[2])\n",
        "        cd = (c+d)*w2*(-1)\n",
        "        exponential_cost = exp(ab)*exp(cd)\n",
        "\n",
        "        if (iou_cost >= iou_thresh and linear_cost>=linear_thresh and exponential_cost>=exp_thresh):\n",
        "            return iou_cost\n",
        "        else :\n",
        "            return 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w6CvRBXgjWK2",
        "colab_type": "text"
      },
      "source": [
        "## The Hungarian Algorithm\n",
        "We can now use the previous code from the workshop to track bounding boxes!\n",
        "\n",
        "* Create an **associate()** function that takes **two lists of boxes** (time t-1 and time t) and that outputs **the matches, the new detections, and the unmatched tracks**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SXwhIsjd6kFY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from scipy.optimize import linear_sum_assignment\n",
        "\n",
        "def associate(old_boxes, new_boxes):\n",
        "    \"\"\"\n",
        "    old_boxes will represent the former bounding boxes (at time 0)\n",
        "    new_boxes will represent the new bounding boxes (at time 1)\n",
        "    Function goal: Define a Hungarian Matrix with IOU as a metric and return, for each box, an id\n",
        "    \"\"\"\n",
        "    # Define a new IOU Matrix nxm with old and new boxes\n",
        "    iou_matrix = np.zeros((len(old_boxes),len(new_boxes)),dtype=np.float32)\n",
        "\n",
        "    # Go through boxes and store the IOU value for each box \n",
        "    # You can also use the more challenging cost but still use IOU as a reference for convenience (use as a filter only)\n",
        "    for i,old_box in enumerate(old_boxes):\n",
        "        for j,new_box in enumerate(new_boxes):\n",
        "            iou_matrix[i][j] = box_iou(old_box, new_box)\n",
        "            #iou_matrix[i][j] = hungarian_cost(old_box, new_box)\n",
        "\n",
        "    # Call for the Hungarian Algorithm\n",
        "    hungarian_row, hungarian_col = linear_sum_assignment(-iou_matrix)\n",
        "    hungarian_matrix = np.array(list(zip(hungarian_row, hungarian_col)))\n",
        "\n",
        "    # Create new unmatched lists for old and new boxes\n",
        "    matches, unmatched_detections, unmatched_trackers = [], [], []\n",
        "\n",
        "    # Go through the Hungarian Matrix, if matched element has IOU < threshold (0.3), add it to the unmatched \n",
        "    # Else: add the match    \n",
        "    for h in hungarian_matrix:\n",
        "        if(iou_matrix[h[0],h[1]]<0.3):\n",
        "            unmatched_trackers.append(old_boxes[h[0]])\n",
        "            unmatched_detections.append(new_boxes[h[1]])\n",
        "        else:\n",
        "            matches.append(h.reshape(1,2))\n",
        "    \n",
        "    if(len(matches)==0):\n",
        "        matches = np.empty((0,2),dtype=int)\n",
        "    else:\n",
        "        matches = np.concatenate(matches,axis=0)\n",
        "    \n",
        "    # Go through old boxes, if no matched detection, add it to the unmatched_old_boxes\n",
        "    for t,trk in enumerate(old_boxes):\n",
        "\t    if(t not in hungarian_matrix[:,0]):\n",
        "\t\t    unmatched_trackers.append(trk)\n",
        "    \n",
        "    # Go through new boxes, if no matched tracking, add it to the unmatched_new_boxes\n",
        "    for d, det in enumerate(new_boxes):\n",
        "        if(d not in hungarian_matrix[:,1]):\n",
        "                unmatched_detections.append(det)\n",
        "    \n",
        "    return matches, unmatched_detections,unmatched_trackers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W_TtyGCW5hvW",
        "colab_type": "text"
      },
      "source": [
        "## Main Loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m4uClkOsM7Mg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def main(input_image):\n",
        "    \"\"\"\n",
        "    Receives an images\n",
        "    Outputs the result image, and a list of obstacle objects \n",
        "    \"\"\"\n",
        "    global stored_obstacles # Will be used to keep track of obstacles information\n",
        "    global idx # Will be used to keep track of id information\n",
        "    # Run obstacle detection\n",
        "    image = copy.deepcopy(input_image)\n",
        "    _, out_boxes = yolo.inference(input_image)\n",
        "    # First iteration: Simply create obstacles and draw them\n",
        "    if (idx == 0):\n",
        "        stored_obstacles = []\n",
        "        for i, box in enumerate(out_boxes): # For all detected boxes\n",
        "            obs = Obstacle(idx, box) # Create an obstacle\n",
        "            left, top, right, bottom = convert_data(box) # Move to x1,y1,x2,y2\n",
        "            cv2.rectangle(image, (left, top), (right, bottom), id_to_color(obs.idx), thickness=10) # Draw the box on the image with ids\n",
        "            image = cv2.putText(image, str(obs.idx),(left - 10,top -10),cv2.FONT_HERSHEY_SIMPLEX, 1,id_to_color(obs.idx),thickness=4)\n",
        "            stored_obstacles.append(obs) # Put every created obstacle in the final list                \n",
        "            idx +=1 # Increase the id for every box\n",
        "        return image, stored_obstacles\n",
        "\n",
        "    elif (idx != 0): # In case we already have obstacles from previous frame, work on association\n",
        "        ## Before calling associate, we must create a list of old obstacles\n",
        "        old_obstacles = [obs.box for obs in stored_obstacles] # Simply get the boxes\n",
        "        matches, unmatched_detections, unmatched_tracks = associate(old_obstacles, out_boxes) # Associate the obstacles\n",
        "        new_obstacles = []\n",
        "\n",
        "        # For every match, change the obstacle value\n",
        "        # Assign the id to the matched id\n",
        "        # Assign the box to the new box\n",
        "        for match in matches:\n",
        "            obs = Obstacle(stored_obstacles[match[0]].idx, out_boxes[match[1]])\n",
        "            new_obstacles.append(obs)\n",
        "        \n",
        "        # Loop through all unmatched detections and add these as obstacles\n",
        "        for new_obs in unmatched_detections:\n",
        "            idx+=1\n",
        "            obs = Obstacle(idx, new_obs)\n",
        "            new_obstacles.append(obs)\n",
        "        \n",
        "        # For every obstacle, draw on the image and return it\n",
        "        for i, obs in enumerate(new_obstacles):\n",
        "            left, top, right, bottom = convert_data(obs.box)\n",
        "            cv2.rectangle(image, (left, top), (right, bottom), id_to_color(obs.idx), thickness=10)\n",
        "            image = cv2.putText(image, str(obs.idx),(left - 10,top - 10),cv2.FONT_HERSHEY_SIMPLEX, 1,id_to_color(obs.idx),thickness=4)                \n",
        "        stored_obstacles = copy.deepcopy(new_obstacles)\n",
        "        return image, stored_obstacles"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4cVeLR137YGf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 368
        },
        "outputId": "a0f90098-7dd1-48a7-f297-c12c6468f2b9"
      },
      "source": [
        "### Call the main loop\n",
        "\n",
        "yolo = YOLO()\n",
        "idx = 0\n",
        "\n",
        "fig=plt.figure(figsize=(100,100))\n",
        "\n",
        "result_images_3 = copy.deepcopy(dataset_images)\n",
        "\n",
        "out_imgs = []\n",
        "\n",
        "for i in range(len(result_images_3)):\n",
        "    out_img, stored_obstacles = main(result_images_3[i])\n",
        "    out_imgs.append(out_img)\n",
        "    fig.add_subplot(1, len(result_images_3), i+1)\n",
        "    plt.imshow(out_imgs[i])\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AU0EM5o_LvAk",
        "colab_type": "text"
      },
      "source": [
        "# Improvements"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aRWklScdLr-7",
        "colab_type": "text"
      },
      "source": [
        "### Changing the Non Maxima Suppression formula\n",
        "\n",
        "In OpenCV DNN, NMS (Non Maxima Suppression) is computer per class, insteaf of on the whole list. For that reason, we may arrive at unwanted results:\n",
        "![](https://user-images.githubusercontent.com/25801568/79720833-01a88180-82ea-11ea-993b-8accd6b7fcc1.png)\n",
        "\n",
        "I have found a new Non-Maxima Suppression formula we can use [on PyImageSearch's page](https://www.pyimagesearch.com/2015/02/16/faster-non-maximum-suppression-python/).\n",
        "\n",
        "We have to adapt it:\n",
        "\n",
        "*   We have (x1,y1,w,h) and we the function takes (x1,y1, x2, y2)\n",
        "*   That's it!\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EZk4Lc1dLzFb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from yolo_nms import *\n",
        "\n",
        "yolo = YOLO()\n",
        "idx = 0\n",
        "\n",
        "fig=plt.figure(figsize=(100,100))\n",
        "\n",
        "result_images_3 = copy.deepcopy(dataset_images)\n",
        "\n",
        "out_imgs = []\n",
        "\n",
        "for i in range(len(result_images_3)):\n",
        "    out_img, stored_obstacles = main(result_images_3[i])\n",
        "    out_imgs.append(out_img)\n",
        "    fig.add_subplot(1, len(result_images_3), i+1)\n",
        "    plt.imshow(out_imgs[i])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ftb6lKOSBJup",
        "colab_type": "text"
      },
      "source": [
        "### Using Age\n",
        "\n",
        "We now have a pretty good tracker! <p>\n",
        "One thing that is not very good is that it relies solely on the detector.\n",
        "If we miss the detection, we miss everything. <p>\n",
        "\n",
        "In this part, we'll introduce two ideas:\n",
        "* False Positive\n",
        "* False Negative<p>\n",
        "\n",
        "A **false positive** means that you detected an obstacle that shouldn't detect.<p>\n",
        "We'll solve it by introducing a **MIN_HIT_STREAK** variable. If the detector detects something once, it is not displayed. If it **detects it twice in a row**, or 3 times in a row (thanks to matching), it is displayed.\n",
        "\n",
        "A **false negative** means that you didn't detect an obstacle that should have been detected.<p>\n",
        "We'll solve it by introducing a **MAX_AGE** variable. If an obstacle is suddently unmatched, we **keep displaying** it. If it is unmatched again, or more times, we remove it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SvLcyXVrBgeq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MIN_HIT_STREAK = 1\n",
        "MAX_UNMATCHED_AGE = 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IeBeNMRWQHG8",
        "colab_type": "text"
      },
      "source": [
        "**Obstacle Class** <p>\n",
        "Let's redefine the Obstacle class to include these values\n",
        "Every obstacle should have:\n",
        "* an id\n",
        "* a box\n",
        "* an age (number of times matched)\n",
        "* an unmatched frame number (number of times unmatched)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QyUG4St3QDTZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Obstacle():\n",
        "    def __init__(self, idx, box, age=1, unmatched_age=0):\n",
        "        self.idx = idx\n",
        "        self.box = box\n",
        "        self.age = age\n",
        "        self.unmatched_age = unmatched_age"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DNK1OJVWQj3G",
        "colab_type": "text"
      },
      "source": [
        "**Main Loop**<p>\n",
        "Now we can redefine the main loop\n",
        "We simply add conditions to display or not an obstacle"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l5UU7mgTQjR2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def main(input_image):\n",
        "    \"\"\"\n",
        "    Receives an images\n",
        "    Outputs the result image, and a list of obstacle objects \n",
        "    \"\"\"\n",
        "    global stored_obstacles # Will be used to keep track of obstacles information\n",
        "    global idx # Will be used to keep track of id information\n",
        "    # Run obstacle detection\n",
        "    image = copy.deepcopy(input_image)\n",
        "    _, out_boxes = yolo.inference(input_image)\n",
        "    \n",
        "    # What we will do will be very similar but we have a second list of obstacles that answer to the conditions\n",
        "    # On first iteration, we only create obstacles with age=1\n",
        "    if (idx == 0):\n",
        "        stored_obstacles = []\n",
        "        for i, box in enumerate(out_boxes):\n",
        "            obs = Obstacle(idx, box) # Create an obstacle with age=1\n",
        "            stored_obstacles.append(obs)                \n",
        "            idx +=1\n",
        "        return image\n",
        "    \n",
        "    # On this case, if the obstacle has already been matched, we display it depending on the MIN_HIT_STREAK variable\n",
        "    elif (idx != 0): # In case we already have obstacles from previous frame, work on association\n",
        "        ## Before calling associate, we must create a list of old obstacles\n",
        "        old_obstacles = [obs.box for obs in stored_obstacles] # Simply get the boxes\n",
        "        matches, unmatched_detections, unmatched_tracks = associate(old_obstacles, out_boxes) # Associate the obstacles\n",
        "        \n",
        "        selected_obstacles = []\n",
        "        # Loop through all matches and add these as obstacles\n",
        "        new_obstacles = []\n",
        "        for match in matches:\n",
        "            obs = Obstacle(stored_obstacles[match[0]].idx, out_boxes[match[1]], stored_obstacles[match[0]].age +1) # Increase the age by 1\n",
        "            new_obstacles.append(obs)\n",
        "            if obs.age >= MIN_HIT_STREAK:\n",
        "                selected_obstacles.append(obs)\n",
        "        \n",
        "        # Loop through all unmatched detections and add these as obstacles\n",
        "        for new_obs in unmatched_detections:\n",
        "            idx +=1\n",
        "            obs = Obstacle(idx, new_obs)\n",
        "            new_obstacles.append(obs)\n",
        "            if obs.age >= MIN_HIT_STREAK:\n",
        "                selected_obstacles.append(obs)\n",
        "\n",
        "        for i, old_obs in enumerate(unmatched_tracks):\n",
        "            if stored_obstacles[i].box == old_obs:\n",
        "                obs = stored_obstacles[i] \n",
        "                obs.unmatched_age +=1\n",
        "                if obs.unmatched_age <= MAX_UNMATCHED_AGE:\n",
        "                    selected_obstacles.append(obs)\n",
        "\n",
        "        # Draw on selected obstacles only\n",
        "        for i, obs in enumerate(selected_obstacles):\n",
        "            left, top, right, bottom = convert_data(obs.box)\n",
        "            cv2.rectangle(image, (left, top), (right, bottom), id_to_color(obs.idx), thickness=10)\n",
        "            image = cv2.putText(image, str(obs.idx),(left,top),cv2.FONT_HERSHEY_SIMPLEX, 1,id_to_color(obs.idx),thickness=4)                \n",
        "\n",
        "        stored_obstacles = copy.deepcopy(new_obstacles)\n",
        "        return image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nE45f0_JQdGQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 368
        },
        "outputId": "3e0bb0f4-4842-444b-9050-31b2775e17cd"
      },
      "source": [
        "yolo = YOLO()\n",
        "idx = 0\n",
        "\n",
        "fig=plt.figure(figsize=(100,100))\n",
        "\n",
        "result_images_3 = copy.deepcopy(dataset_images)\n",
        "\n",
        "out_imgs = []\n",
        "\n",
        "for i in range(len(result_images_3)):\n",
        "    out_img = main(result_images_3[i])\n",
        "    out_imgs.append(out_img)\n",
        "    fig.add_subplot(1, len(result_images_3), i+1)\n",
        "    plt.imshow(out_imgs[i])\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vnjp_IpHF5Hn",
        "colab_type": "text"
      },
      "source": [
        "# Video\n",
        "\n",
        "\n",
        "Now is the time to run on a video. Import the video_0 file and run it in Paris!\n",
        "If you have GPU, it's even better.\n",
        "Otherwise, use a subclip function to run it only on the first seconds"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WAjA7cW8p0gw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from moviepy.editor import VideoFileClip\n",
        "idx = 0\n",
        "detector = YOLO()\n",
        "#video_file = \"/content/drive/My Drive/Think Autonomous/SDC Course/Tracking/Images/video_0.MOV\"\n",
        "#video_file = \"/content/drive/My Drive/Think Autonomous/SDC Course/Tracking/Images/MOT16-13-raw.mp4\" #25 FPS\n",
        "video_file = \"/content/drive/My Drive/Think Autonomous/SDC Course/Tracking/Images/MOT16-14-raw.mp4\" #25 FPS\n",
        "clip = VideoFileClip(video_file).subclip(0,5)\n",
        "white_clip = clip.fl_image(main)\n",
        "%time white_clip.write_videofile(\"/content/drive/My Drive/Think Autonomous/SDC Course/Tracking/Output/movie_track_kf_out.mp4\",audio=False)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}